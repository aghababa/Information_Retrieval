# -*- coding: utf-8 -*-
"""IR_Project_characters.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dn0Lx46VSJa6Fn4R5IexKIxGeL5hZ_Cw
"""

#pip install tslearn

import glob
import numpy as np 
import time
import math
import random
from scipy import linalg as LA
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
from termcolor import colored
import csv
import scipy.io
#import tslearn
#from tslearn.metrics import dtw

from google.colab import drive
drive.mount("/content/gdrive")

"""# Reading and preprocessing data"""

mat = scipy.io.loadmat('/content/gdrive/My Drive/Research/Characters/mixoutALL_shifted.mat')

labels = mat['consts'][0][0][4][0]
labels

data = [0] * len(mat['mixout'][0])
for j in range(len(data)):
    data[j] = [0] * len(mat['mixout'][0][j][0])
    data[j][0] = np.zeros(2)
    for i in range(1, len(mat['mixout'][0][j][0])):
        data[j][i] = data[j][i-1] + mat['mixout'][0][j][:2, i]
    data[j] = np.array(data[j])
        
data = np.array(data, dtype=object)
data.shape

np.random.seed(10)
I = np.random.permutation(len(data))[:300]
data = data[I]

I

"""# DTW distances"""

def calculate_dists_dtw_tslearn(data, path): 
    start_time = time.time() 
    n = len(data)
    A = []
    for i in range(n-1):
        for j in range(i+1, n):
            A.append(tslearn.metrics.dtw(data[i], data[j]))
    A = np.array(A)
    tri = np.zeros((n, n))
    tri[np.triu_indices(n, 1)] = A
    for i in range(1, n):
        for j in range(i):
            tri[i][j] = tri[j][i]
    np.savetxt(path, tri, delimiter=',')

    total_time = time.time() - start_time
    return total_time

path = '/content/gdrive/My Drive/IR Project/Characters/'

# Run just for once
#calculate_dists_dtw_tslearn(data, path=path+'dtw-characters.csv')

dists_dtw = np.array(pd.read_csv(path+'dtw-characters.csv', header=None))
dists_df = pd.read_csv(path+'dtw-characters.csv', header=None)
#dists[:5, :5]
dists_df.head()[[0,1,2,3,4,5,6,7,8,9]]

dists_dtw.shape

ranked_inverted_index_dtw = np.argsort(dists_dtw, 1)

#np.savetxt(path+'ranked_inverted_index_dtw.csv', ranked_inverted_index_dtw, delimiter=',')

ranks_dtw = np.vectorize(int)(np.array(pd.read_csv(path+'ranked_inverted_index_dtw.csv', header=None)))
ranks_dtw

dists_dtw = dists_dtw[I][:,I]
dists_dtw.shape

ranks_dtw = ranks_dtw[I][:, I]
ranks_dtw.shape

"""# Vectorizing trajectories

## Functions
"""

n_x_grids = 50
n_y_grids = 50
path = '/content/gdrive/My Drive/IR Project/Characters/'
sigma_coeff = 3
M = len(data)
reduction_dim = 25
option= 'binary' #'multipass'
max_limit = 50

def getGridsXAndY(data, n_x_grids, n_y_grids, path=None):
    '''
    path = '/content/gdrive/My Drive/IR Project/Characters/'
    return: x, y-linspaces in the rectangle (x_min, y_min), (x_max, y_max)
    '''
    x_min, y_min = np.min([np.min(data[i], axis=0) for i in range(len(data))], axis=0)
    x_max, y_max = np.max([np.max(data[i], axis=0) for i in range(len(data))], axis=0)
    x_margin = 0.01 * (x_max - x_min)
    y_margin = 0.01 * (y_max - y_min)
    x = np.linspace(x_min - x_margin, x_max + x_margin, n_x_grids+1)
    y = np.linspace(y_min - y_margin, y_max + y_margin, n_y_grids+1)
    if path:
        np.savetxt(path+"x.csv", x.reshape(1, len(x)), delimiter=",")
        np.savetxt(path+"y.csv", y.reshape(1, len(y)), delimiter=",")
        np.savetxt(path+"minMaxs.csv", [x_min, y_min, x_max, y_max], delimiter="'")
    return x, y, x_min, y_min, x_max, y_max

x, y, x_min, y_min, x_max, y_max = getGridsXAndY(data, n_x_grids, n_y_grids, path)
n_x_grids = len(x) - 1
n_y_grids = len(y) - 1

def centersSimilarityMatrix(sigma_coeff=5, path=None):
    '''
    Gives the similarity matrix of centers of grids using Gauusian kernel
    '''
    tensor = np.zeros((n_x_grids, n_y_grids, 2))
    for i in range(n_x_grids):
        for j in range(n_y_grids): # range(self.n_y_grids - 1, -1, -1)
            tensor[i][j] = [(x[i]+x[i+1])/2, (y[j]+y[j+1])/2]
    tensor = tensor.reshape(-1,2)
    simMatrix = np.ones((len(tensor), len(tensor)))
    sigma = sigma_coeff * ((y_max-y_min)+(x_max-x_min))/(n_x_grids + n_y_grids)
    for i in range(len(tensor) - 1):
        simMatrix[i][i+1:] = np.exp(-np.sum((tensor[i+1:]-tensor[i])*(tensor[i+1:]-tensor[i]), 
                                             axis=1)/sigma**2)
    for i in range(len(tensor) - 1):
        simMatrix[:,i][i+1:] = simMatrix[i][i+1:]
    return simMatrix

C = centersSimilarityMatrix(sigma_coeff, path=None)
C.shape

def vectorizeLineSegment(LineSegment): 
    '''
    LineSegment: of shape (2, 2); coordinates of start and end are given; [[x1, y1], [x2, y2]]
    return: a binary vector of length n_x_grids x n_y_grids, which shows if 
            LineSegment has occured in each grid or not
    '''
    array = np.zeros((n_x_grids, n_y_grids))
    p1, p2 = LineSegment
    slope = (p2-p1)[1]/((p2-p1)[0] + 1e-10)
    i1 = np.where(x - p1[0] < 0)[0][-1]
    i2 = np.where(x - p2[0] < 0)[0][-1]
    xIdxStart = min(i1, i2)
    xIdxEnd = max(i1, i2)

    if xIdxEnd != xIdxStart:
        ''' dealing with start point of lineSegment'''
        if i1 < i2:
            j_start = np.where(y - p1[1] < 0)[0][-1]
            y_val_end = slope * (x[i1+1] - p1[0]) + p1[1]
        else:
            j_start = np.where(y - p2[1] < 0)[0][-1]
            y_val_end = slope * (x[i2+1] - p1[0]) + p1[1]
        j_end = np.where(y - y_val_end < 0)[0][-1]
        array[xIdxStart][np.arange(min(j_start, j_end), max(j_start, j_end)+1)] = 1

        ''' dealing with end point of lineSegment'''
        if i1 < i2:
            j_end = np.where(y - p2[1] < 0)[0][-1]
            y_val_start = slope * (x[i2] - p1[0]) + p1[1]
        else:
            j_end = np.where(y - p1[1] < 0)[0][-1]
            y_val_start = slope * (x[i1] - p1[0]) + p1[1]
        j_start = np.where(y - y_val_start < 0)[0][-1]
        array[xIdxEnd][np.arange(min(j_start, j_end), max(j_start, j_end)+1)] = 1

        ''' dealing with non-endpoints of lineSegment'''
        for i in range(xIdxStart+1, xIdxEnd):
            y_val_start = slope * (x[i] - p1[0]) + p1[1]
            y_val_end = slope * (x[i+1] - p1[0]) + p1[1]
            j_start = np.where(y - y_val_start < 0)[0][-1]
            j_end = np.where(y - y_val_end < 0)[0][-1]
            array[i][np.arange(min(j_start, j_end), max(j_start, j_end)+1)] = 1
    else:
        y_val_start = min(p1[1], p2[1])
        y_val_end = max(p1[1], p2[1])
        j_start = np.where(y - y_val_start < 0)[0][-1]
        j_end = np.where(y - y_val_end < 0)[0][-1]
        array[xIdxStart][np.arange(min(j_start, j_end), max(j_start, j_end)+1)] = 1
    return array.reshape(n_x_grids * n_y_grids)

def vectorizeTrajectory(trajectory, option): # T --> T_tilda
    '''
    trajectory: of shape (m, 2)
    return: a vector of length n_x_grids x n_y_grids, which shows the number 
            of occurences of trajectory in each grid
    '''
    A = [vectorizeLineSegment(trajectory[i:i+2]) for i in range(len(trajectory) - 1)]
    if option == 'multipass':
        return np.sum(A, axis=0)
    elif option == 'binary':
        return np.sign(np.sum(A, axis=0))


def vectorizeData(data, option): # data --> D_tilda
    ''' 
    vectorizes vectorizeTrajectory() over data 
    '''
    mappedData = [vectorizeTrajectory(data[i], option) for i in range(len(data))]
    return np.array(mappedData).T

start_time = time.time()
print("option:", option)
D_tilda = vectorizeData(data, option)
print(time.time() - start_time)

import scipy 

def dimReductionData(data, option, reduction_dim, path=None):
    '''
    Performs dimentionality reduction using SVD of similarity matrix C of centers of grids
    path='/content/gdrive/My Drive/IR Project/'
    '''
    D_tilda = vectorizeData(data, option)
    Delta, P = np.linalg.eigh(C, UPLO='L')
    I = np.where(Delta <= 1e-15)[0]
    Delta[I] = 0
    Delta_0 = Delta 
    D_tilda_tilda = np.diag(np.sqrt(Delta_0)) @ P.T @ D_tilda
    U, S, Vt = scipy.linalg.svd(D_tilda_tilda)
    U_e = U[:, :reduction_dim] 
    if path:
        np.savetxt(path+"Delta_0_sqrt_"+option+".csv", np.sqrt(Delta_0).reshape(1,len(Delta_0)), delimiter=',')
        np.savetxt(path+"Pt_"+option+".csv", P.T, delimiter=',')
        np.savetxt(path+"U_e_"+option+".csv", U_e, delimiter=',')
    return np.sqrt(Delta_0), P.T, U_e, U_e.T @ D_tilda_tilda

s = time.time()
print('option:', option)
Delta_0_sqrt, Pt, U_e, reducedData = dimReductionData(data, option, reduction_dim, path)

print(reducedData.shape)
print(time.time() - s)

def gridAggregateDist(traj1, traj2, option):
    '''
    Calculates the Grid Aggregated Distance between two trajectories traj1 and traj2 
    with respect to centers matrix C; normalized by the number of grids
    '''
    traj_1 = vectorizeTrajectory(traj1, option)
    traj_2 = vectorizeTrajectory(traj2, option)
    dist = ((traj_1 - traj_2).T @ Pt.T @ np.diag(Delta_0_sqrt**2) @ Pt @ (traj_1 - traj_2))
    return np.sqrt(dist)/(n_x_grids * n_y_grids)

def gridAggregateDistVectorized(traj_vectorized, data_vectorized, option):
    '''
    Calculates the Grid Aggregated Distance between traj_vectorized and data_vectorized 
    (which is an array of vectorized trajectories) with respect to centers matrix C; 
    normalized by the number of grids
    '''
    c = np.diag(Delta_0_sqrt) @ Pt @ (data_vectorized - traj_vectorized).T
    dists = np.sum(c * c, 0)
    return np.sqrt(dists)/(n_x_grids * n_y_grids)

def dimReducedQuery(query, option):
    Min_x, Min_y = np.min(query, axis=0)
    Max_x, Max_y = np.max(query, axis=0)
    if (x[0] < Min_x and y[0] < Min_y and Max_x < x[-1] and Max_y < y[-1]):
        T_q_tilda = vectorizeTrajectory(query, option=option) # vectorizedQuery
        T_q_tilda_tilda = np.diag(Delta_0_sqrt) @ Pt @ T_q_tilda
        T_q_e_tilda_tilda = U_e.T @ T_q_tilda_tilda
        return T_q_e_tilda_tilda 
    else:
        print(colored(f"The query is not in the specified range of data.\n \
        Please enter a query in the valid rectangular area: \n \
        {[x[0], x[-1]]}x{[y[0], y[-1]]}", "yellow"))
        return

for i in range(len(data)):
    plt.plot(data[i][:,0], data[i][:,1])
    plt.plot(x, y)
plt.show()

def queryRankList(query, reduction_dim, option, max_limit, k=None, r=None): 
    '''
    returns a list; the indices of the trajectories to be retrieved for the query
    '''
    mappedReducedQuery = dimReducedQuery(query, option=option)
    distQueryFromReducedData = LA.norm(reducedData - mappedReducedQuery.reshape(reduction_dim,1), 
                                       axis=0) / (n_x_grids * n_y_grids)
    distQueryFromReducedDataSortedIndices = np.argsort(distQueryFromReducedData)
    if k:
        return distQueryFromReducedDataSortedIndices[:k]
    elif r:
        idx = np.where(distQueryFromReducedData < r)[0]
        return idx[:max_limit]
    else:
        print(colored("Please specify k or r", "yellow"))
        return

def dataRankedLists(data_vectorized, reducedData, option, path=None): 
    # get real dists of data points with grid aggregated distance
    n = len(data)
    distsData = np.zeros((n, n))
    for j in range(n-1):
        distsData[j][j+1:] = gridAggregateDistVectorized(data_vectorized[j], 
                                                         data_vectorized[j+1:], 
                                                         option=option)
    distsData = distsData + distsData.T
    distsDataArgSorts = np.argsort(distsData)
    if path:
        np.savetxt(path+"distsData_"+option+".csv", distsData, delimiter=",")
        np.savetxt(path+"distsDataArgSorts_"+option+".csv", distsDataArgSorts, delimiter=",")

    # get dists of data points after dim reduction
    distsReducedData = np.zeros((n,n))
    for j in range(n-1):
        a = LA.norm(reducedData[j] - reducedData[j+1:], axis=1)/(n_x_grids * n_y_grids)
        distsReducedData[j][j+1:] = a
    distsReducedData = distsReducedData + distsReducedData.T
    distsReducedDataArgSorts = np.argsort(distsReducedData)
    if path:
        np.savetxt(path+"distsReducedData_"+option+".csv", distsReducedData, delimiter=",")
        np.savetxt(path+"distsReducedDataArgSorts_"+option+".csv", distsReducedDataArgSorts, delimiter=",")
    return distsData, distsDataArgSorts, distsReducedData, distsReducedDataArgSorts

start_time = time.time()
print('option:', option)
distsData, distsDataArgSorts, distsReducedData, distsReducedDataArgSorts = \
                        dataRankedLists(D_tilda.T, reducedData.T, option, path)
print(time.time() - start_time)

sum(np.argsort(distsData[0]) != np.argsort(distsReducedData[0]))

data_ranked_list_dtw = np.argsort(dists_dtw)
data_ranked_list_dtw.shape

def precisionAndRecallData(data, N, max_limit, distsData, distsDataArgSorts, 
                           distsReducedData, distsReducedDataArgSorts, k=None, r=None): 
    '''
    calculates percision and recall for all data points as well as 
    the average of percision and recall for all data @N (Note: This is not AP)
    This function is not sensitive to k if k>=N.
    Note: Don't give values for both k and r (one of them should be None).
    '''
    percision = np.zeros(len(data))
    recall = np.zeros(len(data))
    if (k and r):
        for j in range(len(data)):
            c = 0
            for i in range(N):
                if distsReducedDataArgSorts[j][i] in distsDataArgSorts[j][:N]:
                    c += 1
            percision[j] = c/N
            recall[j] = c/N
        percision_k = percision.copy()
        recall_k = recall.copy()
        for j in range(len(data)):
            d = 0
            idx = np.where(distsData[j] < r)[0][:min(max_limit, N)]
            idx_red = np.where(distsReducedData[j] < r)[0][:min(max_limit, N)]
            for i in range(len(idx_red)):
                if idx_red[i] in idx:
                    d += 1
            percision[j] = d/len(idx_red)
            recall[j] = d/len(idx)
        return {f"k={k}:": [percision_k, recall_k, np.mean(percision_k), np.mean(recall_k)],
                f"r={r}:": [percision, recall, np.mean(percision), np.mean(recall)]}
    elif k:
        for j in range(len(data)):
            c = 0
            for i in range(N):
                if distsReducedDataArgSorts[j][i] in distsDataArgSorts[j][:N]:
                    c += 1
            percision[j] = c/N
            recall[j] = c/N
        return percision, recall, np.mean(percision), np.mean(recall)
    elif r:
        for j in range(len(data)):
            c = 0
            idx = np.where(distsData[j] < r)[0]
            idx_red = np.where(distsReducedData[j] < r)[0]
            I = np.argsort(distsData[j][idx])
            idx = idx[I][:min(max_limit, N)]
            J = np.argsort(distsReducedData[j][idx_red])
            idx_red = idx_red[J][:min(max_limit, N)]
            for i in range(len(idx_red)):
                if idx_red[i] in idx:
                    c += 1
            percision[j] = c/len(idx_red)
            recall[j] = c/len(idx)
        return percision, recall, np.mean(percision), np.mean(recall)
    else:
        print(colored("Pleas specify the value of k or r.", "yellow"))

"""### Average of percisions of all data points where each is used as kNN queries

With GAD as ground truth
"""

# Note: for kNN queries percision and recall are the same
print("option:", option)
percision = []
recall = []
mean_percision = []
mean_recall = []

for k in [5, 10, 20]:
    a, b, c, d = precisionAndRecallData(data, N=k, max_limit=50, 
                            distsData=distsData, distsDataArgSorts=distsDataArgSorts, 
                            distsReducedData=distsReducedData, 
                            distsReducedDataArgSorts=distsReducedDataArgSorts, k=k, r=None)
    percision.append(a)
    recall.append(b)
    mean_percision.append(c)
    mean_recall.append(d)

#print(colored(f"percision: {percision}", 'yellow'))
#print("recall:", recall)
print(colored(f"mean_percision: {np.array(mean_percision)}", 'yellow'))
print("mean_recall:", np.array(mean_recall))

# Note: for kNN queries percision and recall are the same
print("option:", option)
percision = []
recall = []
mean_percision = []
mean_recall = []

for k in [5, 10, 20]:
    a, b, c, d = precisionAndRecallData(data, N=k, max_limit=50,  
                            distsData=distsData, distsDataArgSorts=distsDataArgSorts, 
                            distsReducedData=distsReducedData, 
                            distsReducedDataArgSorts=distsReducedDataArgSorts, k=k, r=None)
    percision.append(a)
    recall.append(b)
    mean_percision.append(c)
    mean_recall.append(d)

#print(colored(f"percision: {percision}", 'yellow'))
#print("recall:", recall)
print(colored(f"mean_percision: {np.array(mean_percision)}", 'yellow'))
print("mean_recall:", np.array(mean_recall))

"""With dtw as ground truth"""

np.max(dists_dtw/1e5), np.mean(dists_dtw/1e5), np.max(distsData), np.mean(distsData)

"""dtw"""

# Note: for kNN queries percision and recall are the same
print("option:", option)
percision = []
recall = []
mean_percision = []
mean_recall = []

for k in [5, 10, 20]:
    a, b, c, d = precisionAndRecallData(data, N=k, max_limit=50, 
                            distsData=dists_dtw/1e5, distsDataArgSorts=data_ranked_list_dtw, 
                            distsReducedData=distsReducedData, 
                            distsReducedDataArgSorts=distsReducedDataArgSorts, k=k, r=None)
    percision.append(a)
    recall.append(b)
    mean_percision.append(c)
    mean_recall.append(d)

#print(colored(f"percision: {percision}", 'yellow'))
#print("recall:", recall)
print(colored(f"mean_percision: {np.array(mean_percision)}", 'yellow'))
print("mean_recall:", np.array(mean_recall))

"""dtw"""

# Note: for kNN queries percision and recall are the same
print("option:", option)
percision = []
recall = []
mean_percision = []
mean_recall = []

for k in [5, 10, 20]:
    a, b, c, d = precisionAndRecallData(data, N=k, max_limit=50, 
                            distsData=dists_dtw/1e4, distsDataArgSorts=data_ranked_list_dtw, 
                            distsReducedData=distsReducedData, 
                            distsReducedDataArgSorts=distsReducedDataArgSorts, k=k, r=None)
    percision.append(a)
    recall.append(b)
    mean_percision.append(c)
    mean_recall.append(d)

#print(colored(f"percision: {percision}", 'yellow'))
#print("recall:", recall)
print(colored(f"mean_percision: {np.array(mean_percision)}", 'yellow'))
print("mean_recall:", np.array(mean_recall))

"""### Averages of percisions and recalls of all data points where each is used as range queries"""

np.max(distsData), np.max(distsReducedData)

"""With GAD as ground truth"""

print("option:", option)
percision = []
recall = []
mean_percision = []
mean_recall = []

for r in [0.002, 0.005, 0.01]:
    a, b, c, d = precisionAndRecallData(data, N=10, max_limit=50, 
                            distsData=distsData, distsDataArgSorts=distsDataArgSorts, 
                            distsReducedData=distsReducedData, 
                            distsReducedDataArgSorts=distsReducedDataArgSorts, k=None, r=r)
    percision.append(a)
    recall.append(b)
    mean_percision.append(c)
    mean_recall.append(d)

#print(colored(f"percision: {percision}", 'yellow'))
#print("recall:", recall)
print(colored(f"mean_percision: {np.array(mean_percision)}", 'yellow'))
print("mean_recall:", np.array(mean_recall))

print("option:", option)
percision = []
recall = []
mean_percision = []
mean_recall = []

for r in [0.002, 0.005, 0.01]:
    a, b, c, d = precisionAndRecallData(data, N=10, max_limit=50,
                            distsData=distsData, distsDataArgSorts=distsDataArgSorts, 
                            distsReducedData=distsReducedData, 
                            distsReducedDataArgSorts=distsReducedDataArgSorts, k=None, r=r)
    percision.append(a)
    recall.append(b)
    mean_percision.append(c)
    mean_recall.append(d)

#print(colored(f"percision: {percision}", 'yellow'))
#print("recall:", recall)
print(colored(f"mean_percision: {np.array(mean_percision)}", 'yellow'))
print("mean_recall:", np.array(mean_recall))

""" With dtw as ground truth"""

print("option:", option)
percision = []
recall = []
mean_percision = []
mean_recall = []

for r in [0.002, 0.005, 0.01]:
    a, b, c, d = precisionAndRecallData(data, N=10, max_limit=50, 
                            distsData=dists_dtw/1e5, distsDataArgSorts=data_ranked_list_dtw, 
                            distsReducedData=distsReducedData, 
                            distsReducedDataArgSorts=distsReducedDataArgSorts, k=None, r=r)
    percision.append(a)
    recall.append(b)
    mean_percision.append(c)
    mean_recall.append(d)

#print(colored(f"percision: {percision}", 'yellow'))
#print("recall:", recall)
print(colored(f"mean_percision: {np.array(mean_percision)}", 'yellow'))
print("mean_recall:", np.array(mean_recall))

print("option:", option)
percision = []
recall = []
mean_percision = []
mean_recall = []

for r in [0.002, 0.005, 0.01]:
    a, b, c, d = precisionAndRecallData(data, N=10, max_limit=50, 
                            distsData=dists_dtw/1e4, distsDataArgSorts=data_ranked_list_dtw, 
                            distsReducedData=distsReducedData, 
                            distsReducedDataArgSorts=distsReducedDataArgSorts, k=None, r=r)
    percision.append(a)
    recall.append(b)
    mean_percision.append(c)
    mean_recall.append(d)

#print(colored(f"percision: {percision}", 'yellow'))
#print("recall:", recall)
print(colored(f"mean_percision: {np.array(mean_percision)}", 'yellow'))
print("mean_recall:", np.array(mean_recall))

def precisionAndRecallDataPoint(index, N, max_limit, distsData, distsDataArgSorts, 
                                distsReducedData, distsReducedDataArgSorts, k=None, r=None): 
    '''
    calculates percision and recall for one data point (as a query) @ N
    index: the index of data point
    This function is not sensitive to k if k >= N
    '''
    if k:
        c = 0
        for i in range(N):
            if distsReducedDataArgSorts[index][i] in distsDataArgSorts[index][:N]:
                c += 1
        percision = c/N
        recall = c/N
        return percision, recall
    elif r:
        c = 0
        idx = np.where(distsData[index] < r)[0]
        idx_red = np.where(distsReducedData[index] < r)[0]
        I = np.argsort(distsData[index][idx])
        idx = idx[I][:min(max_limit, N)]
        J = np.argsort(distsReducedData[index][idx_red])
        idx_red = idx_red[J][:min(max_limit, N)]
        for i in range(len(idx_red)):
            if idx_red[i] in idx:
                c += 1
        percision = c/len(idx_red)
        recall = c/len(idx)
        return percision, recall
    else:
        print(colored("Pleas specify the value of k or r.", "yellow"))

def averagePrecisionData(data, path, max_limit, reduction_dim, reducedData,  
                         option, k=None, r=None, dtw=False):
    '''
    calculates average percision (AP) for all data points as well as 
    the average of AP on all data points as queries (i.e. MAP)
    path='/content/gdrive/My Drive/IR Project/'
    Note: Don't give values for both k and r (one of them should be None).
    '''
    if dtw:
        distsData = dists_dtw/1e5
        distsDataArgSorts = data_ranked_list_dtw
    else:
        distsData = np.array(pd.read_csv(path+"distsData_"+option+".csv", header=None))
        distsDataArgSorts = np.array(pd.read_csv(path+"distsDataArgSorts_"+option+".csv", header=None))
    
    distsReducedData = np.array(pd.read_csv(path+"distsReducedData_"+option+".csv", header=None))
    distsReducedDataArgSorts = np.array(pd.read_csv(path+"distsReducedDataArgSorts_"+option+".csv", header=None))
    n = len(data)
    AP = np.zeros(n)
    if k:
        AP_temp = np.zeros((k, n))
        for i in range(k):
            percision, _, _, _ = precisionAndRecallData(data, i+1, max_limit,
                                    distsData, distsDataArgSorts, distsReducedData, 
                                    distsReducedDataArgSorts, k=k, r=r)
            for t in range(n):
                if distsReducedDataArgSorts[t][i] in distsDataArgSorts[t]:
                    AP_temp[i][t] = percision[t] 

        AP = np.mean(AP_temp, 0)
        MAP = np.mean(AP)
        return AP, MAP
    elif r:
        for t in range(n):
            AP_temp = []
            s = len(queryRankList(data[t], reduction_dim, option, max_limit, k=None, r=r))
            for i in range(s):
                percision, _ = precisionAndRecallDataPoint(t, i+1, max_limit, 
                                distsData, distsDataArgSorts, distsReducedData, 
                                    distsReducedDataArgSorts, k=None, r=r)
                idx = np.where(distsData[t] < r)[0]
                idx_red = np.where(distsReducedData[t] < r)[0]
                #
                I = np.argsort(distsData[t][idx])
                idx = idx[I][:min(max_limit, i+1)]
                J = np.argsort(distsReducedData[t][idx_red])
                idx_red = idx_red[J][:min(max_limit, i+1)]
                if i < len(idx_red):
                    if idx_red[i] in idx:
                        AP_temp.append(percision)
            AP[t] = np.sum(AP_temp)/s
        MAP = np.mean(AP)
        return AP, MAP

"""### AP and MAP for kNN queries

#### binary
"""

start_time = time.time()
print("option:", option)
AP = []
MAP = []

for k in [5, 10, 20]:
    a, b = averagePrecisionData(data, path, max_limit, reduction_dim, 
                                reducedData, option, k=k, r=None, dtw=False)
    AP.append(a)
    MAP.append(b)

#print(f"AP: {np.array(AP)}")
print(colored(f"MAP: {np.array(MAP)}", 'yellow'))
print("Total time:", time.time() - start_time)

"""dtw"""

print("option:", option)
start_time = time.time()
AP = []
MAP = []

for k in [5, 10, 20]:
    a, b = averagePrecisionData(data, path, max_limit, reduction_dim, 
                                reducedData, option, k=k, r=None, dtw=True)
    AP.append(a)
    MAP.append(b)

#print(f"AP: {np.array(AP)}")
print(colored(f"MAP: {np.array(MAP)}", 'yellow'))
print("Total time:", time.time() - start_time)

"""#### Multipass"""

start_time = time.time()
print("option:", option)
AP = []
MAP = []

for k in [5, 10, 20]:
    a, b = averagePrecisionData(data, path, max_limit, reduction_dim, 
                                reducedData, option, k=k, r=None, dtw=False)
    AP.append(a)
    MAP.append(b)

#print(f"AP: {np.array(AP)}")
print(colored(f"MAP: {np.array(MAP)}", 'yellow'))
print("Total time:", time.time() - start_time)

"""dtw"""

print("option:", option)
start_time = time.time()
AP = []
MAP = []

for k in [5, 10, 20]:
    a, b = averagePrecisionData(data, path, max_limit, reduction_dim, 
                                reducedData, option, k=k, r=None, dtw=True)
    AP.append(a)
    MAP.append(b)

#print(f"AP: {np.array(AP)}")
print(colored(f"MAP: {np.array(MAP)}", 'yellow'))
print("Total time:", time.time() - start_time)

"""### AP and MAP for range queries

#### binary
"""

start_time = time.time()
print("option:", option)
AP = []
MAP = []

for r in [0.002, 0.005, 0.01]:
    s = time.time()
    a, b = averagePrecisionData(data, path, max_limit, reduction_dim, 
                                reducedData, option, k=None, r=r, dtw=False)
    AP.append(a)
    MAP.append(b)
    print(f"time for {r}: {time.time() - s}")
    
#print(f"AP: {np.array(AP)}")
print(colored(f"MAP: {np.array(MAP)}", 'yellow'))
print(time.time() - start_time)

"""dtw"""

start_time = time.time()
print("option:", option)
AP = []
MAP = []

for r in [0.002, 0.005, 0.01]:
    s = time.time()
    a, b = averagePrecisionData(data, path, max_limit, reduction_dim, 
                                reducedData, option, k=None, r=r, dtw=True)
    AP.append(a)
    MAP.append(b)
    print(f"time for {r}: {time.time() - s}")
    
#print(f"AP: {np.array(AP)}")
print(colored(f"MAP: {np.array(MAP)}", 'yellow'))
print(time.time() - start_time)

"""#### Multipass"""

start_time = time.time()
print("option:", option)
AP = []
MAP = []

for r in [0.002, 0.005, 0.01]:
    s = time.time()
    a, b = averagePrecisionData(data, path, max_limit, reduction_dim, 
                                reducedData, option, k=None, r=r, dtw=False)
    AP.append(a)
    MAP.append(b)
    print(f"time for {r}: {time.time() - s}")
    
#print(f"AP: {np.array(AP)}")
print(colored(f"MAP: {np.array(MAP)}", 'yellow'))
print(time.time() - start_time)

"""dtw"""

start_time = time.time()
print("option:", option)
AP = []
MAP = []

for r in [0.002, 0.005, 0.01]:
    s = time.time()
    a, b = averagePrecisionData(data, path, max_limit, reduction_dim, 
                                reducedData, option, k=None, r=r, dtw=True)
    AP.append(a)
    MAP.append(b)
    print(f"time for {r}: {time.time() - s}")
    
#print(f"AP: {np.array(AP)}")
print(colored(f"MAP: {np.array(MAP)}", 'yellow'))
print(time.time() - start_time)

"""## nDCG"""

def nDCG(data, path, max_limit, reduction_dim, option, N, k=None, r=None, dtw=False):
    '''
    calculates average nDCG @N for all data points as well as 
    the average of nDCG @N on all data points as queries;
    N should be <= k;
    path='/content/gdrive/My Drive/IR Project/Characters/'
    Note: Don't give values for both k and r (one of them should be None).
    '''
    if dtw:
        distsData = dists_dtw/1e5
        distsDataArgSorts = data_ranked_list_dtw
    else:
        distsData = np.array(pd.read_csv(path+"distsData_"+option+".csv", header=None))
        distsDataArgSorts = np.array(pd.read_csv(path+"distsDataArgSorts_"+option+".csv", header=None))
        
    distsReducedData = np.array(pd.read_csv(path+"distsReducedData_"+option+".csv", header=None))
    distsReducedDataArgSorts = np.array(pd.read_csv(path+"distsReducedDataArgSorts_"+option+".csv", header=None))
    DCG = np.zeros(len(data))
    if k: 
        iDCG = np.sum([(k-i)/math.log(i+2, 2) for i in range(k)])
        for j in range(len(data)):
            rel = np.zeros(N)
            for i in range(N):
                if distsReducedDataArgSorts[j][i] in distsDataArgSorts[j]:
                    if k - distsDataArgSorts[j].tolist().index(distsReducedDataArgSorts[j][i]) > 0:
                        rel[i] = k - distsDataArgSorts[j].tolist().index(distsReducedDataArgSorts[j][i])
            DCG[j] = np.sum([rel[i]/math.log(i+2, 2) for i in range(N)])
        return DCG/iDCG, np.mean(DCG/iDCG)
    elif r:
        relevance = list(reversed(range(1, N+1)))
        iDCG = np.sum([relevance[i]/math.log(i+2, 2) for i in range(N)])
        for j in range(len(data)):
            I = np.where(distsData[j] < r)[0]
            relevant = distsDataArgSorts[j][:len(I)]
            J = np.where(distsReducedData[j] < r)[0]
            retrieved = distsReducedDataArgSorts[j][:len(J)]
            rel = np.zeros(N)
            for i in range(N):
                if distsReducedDataArgSorts[j][i] in distsReducedDataArgSorts[j][:len(I)]:
                    if N - distsDataArgSorts[j].tolist().index(distsReducedDataArgSorts[j][i]) > 0:
                        rel[i] = N - distsDataArgSorts[j].tolist().index(distsReducedDataArgSorts[j][i])
            DCG[j] = np.sum([rel[i]/math.log(i+2, 2) for i in range(N)])
        return DCG/iDCG, np.mean(DCG/iDCG)

"""### nDCG for binary"""

# N <= k
print("option:", option)
nDCGs = []
for k in [5, 10, 20]:
    nDCGs.append(nDCG(data, path, max_limit, reduction_dim, option, N=5, k=k, 
                      r=None, dtw=False)[1])
print(colored(f'nDCGs: {np.array(nDCGs)}', 'yellow'))

nDCGs = []
for k in [5, 10, 20]:
    nDCGs.append(nDCG(data, path, max_limit, reduction_dim, option, N=10, k=k, 
                      r=None, dtw=False)[1])
print(colored(f'nDCGs: {np.array(nDCGs)}', 'yellow'))

"""dtw"""

# N <= k
print("option:", option)
nDCGs = []
for k in [5, 10, 20]:
    nDCGs.append(nDCG(data, path, max_limit, reduction_dim, option, N=5, k=k, 
                      r=None, dtw=True)[1])
print(colored(f'nDCGs: {np.array(nDCGs)}', 'yellow'))

nDCGs = []
for k in [5, 10, 20]:
    nDCGs.append(nDCG(data, path, max_limit, reduction_dim, option, N=10, k=k, 
                      r=None, dtw=True)[1])
print(colored(f'nDCGs: {np.array(nDCGs)}', 'yellow'))

print("option:", option)
nDCGs = []
for r in [0.002, 0.005, 0.01]:
    nDCGs.append(nDCG(data, path, max_limit, reduction_dim, option, N=5, k=None,
                      r=r, dtw=False)[1])
print(colored(f'nDCGs@5: {np.array(nDCGs)}', 'yellow'))

nDCGs = []
for r in [0.002, 0.005, 0.01]:
    nDCGs.append(nDCG(data, path, max_limit, reduction_dim, option, N=10, k=None,
                      r=r, dtw=False)[1])
print(colored(f'nDCGs@10: {np.array(nDCGs)}', 'yellow'))

"""dtw"""

print("option:", option)
nDCGs = []
for r in [0.002, 0.005, 0.01]:
    nDCGs.append(nDCG(data, path, max_limit, reduction_dim, option, N=5, k=None,
                      r=r, dtw=True)[1])
print(colored(f'nDCGs@5: {np.array(nDCGs)}', 'yellow'))

nDCGs = []
for r in [0.002, 0.005, 0.01]:
    nDCGs.append(nDCG(data, path, max_limit, reduction_dim, option, N=10, k=None,
                      r=r, dtw=True)[1])
print(colored(f'nDCGs@10: {np.array(nDCGs)}', 'yellow'))

"""### nDCG for multipass"""

# N <= k
print("option:", option)
nDCGs = []
for k in [5, 10, 20]:
    nDCGs.append(nDCG(data, path, max_limit, reduction_dim, option, N=5, k=k, 
                      r=None, dtw=False)[1])
print(colored(f'nDCGs: {np.array(nDCGs)}', 'yellow'))

nDCGs = []
for k in [5, 10, 20]:
    nDCGs.append(nDCG(data, path, max_limit, reduction_dim, option, N=10, k=k, 
                      r=None, dtw=False)[1])
print(colored(f'nDCGs: {np.array(nDCGs)}', 'yellow'))

"""dtw"""

# N <= k
print("option:", option)
nDCGs = []
for k in [5, 10, 20]:
    nDCGs.append(nDCG(data, path, max_limit, reduction_dim, option, N=5, k=k, 
                      r=None, dtw=True)[1])
print(colored(f'nDCGs: {np.array(nDCGs)}', 'yellow'))

nDCGs = []
for k in [5, 10, 20]:
    nDCGs.append(nDCG(data, path, max_limit, reduction_dim, option, N=10, k=k, 
                      r=None, dtw=True)[1])
print(colored(f'nDCGs: {np.array(nDCGs)}', 'yellow'))

print("option:", option)
nDCGs = []
for r in [0.002, 0.005, 0.01]:
    nDCGs.append(nDCG(data, path, max_limit, reduction_dim, option, N=5, k=None,
                      r=r, dtw=False)[1])
print(colored(f'nDCGs@5: {np.array(nDCGs)}', 'yellow'))

nDCGs = []
for r in [0.002, 0.005, 0.01]:
    nDCGs.append(nDCG(data, path, max_limit, reduction_dim, option, N=10, k=None,
                      r=r, dtw=False)[1])
print(colored(f'nDCGs@10: {np.array(nDCGs)}', 'yellow'))

"""dtw"""

print("option:", option)
nDCGs = []
for r in [0.002, 0.005, 0.01]:
    nDCGs.append(nDCG(data, path, max_limit, reduction_dim, option, N=5, k=None,
                      r=r, dtw=True)[1])
print(colored(f'nDCGs@5: {np.array(nDCGs)}', 'yellow'))

nDCGs = []
for r in [0.002, 0.005, 0.01]:
    nDCGs.append(nDCG(data, path, max_limit, reduction_dim, option, N=10, k=None,
                      r=r, dtw=True)[1])
print(colored(f'nDCGs@10: {np.array(nDCGs)}', 'yellow'))

